import streamlit as st
import requests
import json
import re
from bs4 import BeautifulSoup
import time
import os
from datetime import datetime
import glob

# è®¾ç½®é¡µé¢é…ç½® - å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤
st.set_page_config(
    page_title="å¼€æºé¡¹ç›®èµ„è®¯",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items=None
)

# åŠ è½½è‡ªå®šä¹‰CSS
with open('.streamlit/style.css') as f:
    st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

# åˆ›å»ºæ–‡ç« ä¿å­˜ç›®å½•
ARTICLES_DIR = "articles"
if not os.path.exists(ARTICLES_DIR):
    os.makedirs(ARTICLES_DIR)

def extract_github_info(text):
    """ä»APIå“åº”ä¸­æå–GitHubé¡¹ç›®ä¿¡æ¯"""
    try:
        data = json.loads(text)
        return data.get('projects', []), data.get('companies', []), data.get('news_title', ''), data.get('news_content', '')
    except:
        return [], [], '', ''

def clean_repo_name(name):
    """æ¸…ç†ä»“åº“åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
    # ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    name = re.sub(r'[^\w\-./]', '', name)
    return name

def search_github_api(query):
    """ä½¿ç”¨GitHub APIæœç´¢ä»“åº“"""
    try:
        # ä½¿ç”¨GitHub APIæœç´¢
        github_token = st.secrets.get("GITHUB_TOKEN", "")
        headers = {
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        # æŒ‰ç…§starsæ•°é‡æ’åºï¼Œè·å–æœ€ç›¸å…³çš„ç»“æœ
        search_url = f"https://api.github.com/search/repositories?q={query}&sort=stars&order=desc"
        response = requests.get(search_url, headers=headers)
        
        if response.status_code == 200:
            data = response.json()
            if data['items']:
                repo = data['items'][0]  # è·å–ç¬¬ä¸€ä¸ªç»“æœ
                return {
                    'stars': str(repo['stargazers_count']),
                    'forks': str(repo['forks_count']),
                    'url': repo['html_url'],
                    'full_name': repo['full_name']
                }
    except Exception as e:
        st.error(f"GitHub APIæœç´¢å‡ºé”™: {str(e)}")
    return None

def get_github_stats(repo_name):
    """è·å–GitHubä»“åº“çš„starå’Œforkæ•°é‡"""
    try:
        # æ¸…ç†ä»“åº“åç§°
        repo_name = repo_name.strip()
        
        # å¤„ç†ç‰¹æ®Šæƒ…å†µ
        if repo_name.lower() == "deepseek r1":
            repo_name = "deepseek-ai/deepseek-coder"
        
        # å°è¯•ç›´æ¥è®¿é—®ä»“åº“é¡µé¢
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        # å¦‚æœæ˜¯å®Œæ•´çš„ä»“åº“åï¼ˆåŒ…å«/ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
        if '/' not in repo_name:
            # å¯¹äº ollama è¿™æ ·çš„é¡¹ç›®ï¼Œæ·»åŠ ç»„ç»‡å
            if repo_name.lower() == 'ollama':
                repo_name = 'ollama/ollama'
        
        direct_url = f"https://github.com/{repo_name}"
        response = requests.get(direct_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è·å–staræ•°é‡ - æ›´æ–°é€‰æ‹©å™¨
            stars = '0'
            star_element = soup.select_one('a[href$="/stargazers"] > strong')
            if star_element:
                stars = star_element.text.strip()
            
            # è·å–forkæ•°é‡ - æ›´æ–°é€‰æ‹©å™¨
            forks = '0'
            fork_element = soup.select_one('a[href$="/forks"] > strong')
            if fork_element:
                forks = fork_element.text.strip()
            
            # å¦‚æœæ‰¾åˆ°æ•°æ®ï¼Œè¿”å›ç»“æœ
            if stars != '0' or forks != '0':
                # ç§»é™¤åƒåˆ†ä½é€—å·
                stars = stars.replace(',', '')
                forks = forks.replace(',', '')
                return {
                    'stars': stars,
                    'forks': forks,
                    'url': direct_url
                }
        
        # å¦‚æœç›´æ¥è®¿é—®å¤±è´¥ï¼Œå°è¯•æœç´¢
        search_url = f"https://github.com/search?q={repo_name}&type=repositories"
        response = requests.get(search_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            repo_link = soup.select_one('.search-title a:first-child')
            
            if repo_link:
                repo_url = "https://github.com" + repo_link['href']
                response = requests.get(repo_url, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # è·å–staræ•°é‡
                    stars = '0'
                    star_element = soup.select_one('a[href$="/stargazers"] > strong')
                    if star_element:
                        stars = star_element.text.strip().replace(',', '')
                    
                    # è·å–forkæ•°é‡
                    forks = '0'
                    fork_element = soup.select_one('a[href$="/forks"] > strong')
                    if fork_element:
                        forks = fork_element.text.strip().replace(',', '')
                    
                    if stars != '0' or forks != '0':
                        return {
                            'stars': stars,
                            'forks': forks,
                            'url': repo_url
                        }
        
        return {
            'stars': 'æ•°æ®æœªæ‰¾åˆ°',
            'forks': 'æ•°æ®æœªæ‰¾åˆ°',
            'url': f'https://github.com/search?q={repo_name}'
        }
            
    except Exception as e:
        st.error(f"è·å–GitHubæ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return {
            'stars': 'è·å–å¤±è´¥',
            'forks': 'è·å–å¤±è´¥',
            'url': f'https://github.com/search?q={repo_name}'
        }

def analyze_content(text):
    """è°ƒç”¨APIåˆ†æå†…å®¹"""
    url = "https://api.siliconflow.cn/v1/chat/completions"
    headers = {
        "Authorization": "Bearer sk-ydcvskcyyzictsylxplbpqmqlpillcpkqznxclfjyohkefwt",
        "Content-Type": "application/json"
    }
    
    formatted_prompt = f"""è¯·åˆ†æä¸‹é¢çš„æ–‡æœ¬ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š
    1. æåˆ°çš„å¼€æºé¡¹ç›®åç§°
    2. æåˆ°çš„å…¬å¸åç§°
    3. å°†æ–‡æœ¬é‡å†™ä¸ºä¸€ç¯‡èµ„è®¯æŠ¥é“ï¼ˆåŒ…å«æ ‡é¢˜å’Œå†…å®¹ï¼‰
    
    æ–‡æœ¬å†…å®¹ï¼š{text}
    
    è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
    {{
        "projects": ["ä»“åº“å1", "ä»“åº“å2"],
        "companies": ["å…¬å¸1", "å…¬å¸2"],
        "news_title": "æ–°é—»æ ‡é¢˜",
        "news_content": "æ–°é—»å†…å®¹"
    }}"""

    payload = {
        "model": "Qwen/Qwen2.5-72B-Instruct-128K",
        "messages": [{"role": "user", "content": formatted_prompt}],
        "stream": False,
        "max_tokens": 2048,
        "stop": ["<string>"],
        "temperature": 0.7,
        "top_p": 0.7,
        "top_k": 50,
        "frequency_penalty": 0.5,
        "n": 1,
        "response_format": {"type": "json_object"}
    }

    response = requests.post(url, json=payload, headers=headers)
    if response.status_code == 200:
        data = response.json()
        return data['choices'][0]['message']['content'].strip()
    else:
        st.error(f"Error: {response.status_code} - {response.text}")
        return None

def save_article(title, content, projects, companies):
    """ä¿å­˜æ–‡ç« ä¸ºMarkdownæ–‡ä»¶"""
    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{timestamp}_{title[:30]}.md"
    filepath = os.path.join(ARTICLES_DIR, filename)
    
    # æ„å»ºMarkdownå†…å®¹
    markdown_content = f"""# {title}

##### ç›¸å…³é¡¹ç›®
"""
    
    # æ·»åŠ é¡¹ç›®ä¿¡æ¯
    for project in projects:
        stats = get_github_stats(project)
        markdown_content += f"- [{project}]({stats['url']}) - â­ {stats['stars']} | ğŸ”„ {stats['forks']}\n"
    
    # æ·»åŠ å…¬å¸ä¿¡æ¯
    if companies:
        markdown_content += "\n##### ç›¸å…³å…¬å¸\n"
        for company in companies:
            markdown_content += f"- {company}\n"
    
    # æ·»åŠ æ­£æ–‡
    markdown_content += f"\n{content}\n"
    
    # ä¿å­˜æ–‡ä»¶
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(markdown_content)
    
    return filename

def list_articles():
    """è·å–æ‰€æœ‰æ–‡ç« åˆ—è¡¨"""
    articles = []
    for file in glob.glob(os.path.join(ARTICLES_DIR, "*.md")):
        filename = os.path.basename(file)
        # ä»æ–‡ä»¶åä¸­æå–æ—¶é—´å’Œæ ‡é¢˜
        time_str = filename[:15]  # è·å–æ—¶é—´æˆ³éƒ¨åˆ†
        title = filename[16:-3]   # è·å–æ ‡é¢˜éƒ¨åˆ†ï¼ˆå»æ‰.mdï¼‰
        
        # è¯»å–æ–‡ä»¶å†…å®¹è·å–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
        with open(file, 'r', encoding='utf-8') as f:
            full_title = f.readline().strip('# \n')
        
        created_time = datetime.strptime(time_str, "%Y%m%d_%H%M%S")
        articles.append({
            'filename': filename,
            'title': full_title,
            'created_time': created_time,
            'path': file
        })
    
    # æŒ‰æ—¶é—´å€’åºæ’åº
    return sorted(articles, key=lambda x: x['created_time'], reverse=True)

def main():
    # åˆ›å»ºä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.markdown('<div class="sidebar-title">ğŸ“ å¯¼èˆªèœå•</div>', unsafe_allow_html=True)
        
        # å¯¼èˆªé€‰é¡¹
        nav_items = {
            "âœ¨ åˆ›å»ºæ–°æ–‡ç« ": "create",
            "ğŸ“š å…¨éƒ¨æ–‡ç« ": "list"
        }
        
        # ä½¿ç”¨session_stateæ¥ä¿æŒå¯¼èˆªçŠ¶æ€
        if 'nav' not in st.session_state:
            st.session_state.nav = 'create'
            
        for label, value in nav_items.items():
            if st.button(label, key=f"nav_{value}", use_container_width=True):
                st.session_state.nav = value
    
    # æ ¹æ®å¯¼èˆªé€‰æ‹©æ˜¾ç¤ºä¸åŒå†…å®¹
    if st.session_state.nav == "create":
        st.markdown('<h1 class="main-title">âœ¨ åˆ›å»ºæ–°æ–‡ç« </h1>', unsafe_allow_html=True)
        text = st.text_area("è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬å†…å®¹:", height=200)
        col1, col2, col3 = st.columns([1,1,1])
        with col2:
            analyze_button = st.button("ğŸ” å¼€å§‹AIé‡å†™æ•´ç†", use_container_width=True)
        
        if analyze_button and text:
            with st.spinner("ğŸ”„ æ­£åœ¨åˆ†æå†…å®¹..."):
                result = analyze_content(text)
                if result:
                    projects, companies, news_title, news_content = extract_github_info(result)
                    
                    # æ˜¾ç¤ºç”Ÿæˆçš„æ–°é—»
                    if news_title and news_content:
                        st.markdown(f'''
                        <div class="article-card">
                            <h2 class="article-title">{news_title}</h2>
                            <div class="article-meta">
                                <span class="meta-item">ğŸ“… {datetime.now().strftime("%Y-%m-%d %H:%M")}</span>
                                <span class="meta-item">ğŸ‘€ å¼€æºé¡¹ç›®: {len(projects)}ä¸ª</span>
                                <span class="meta-item">ğŸ¢ ç›¸å…³å…¬å¸: {len(companies)}ä¸ª</span>
                            </div>
                            <div class="article-content">{news_content}</div>
                        </div>
                        ''', unsafe_allow_html=True)
                    
                    # æ˜¾ç¤ºé¡¹ç›®ä¿¡æ¯ï¼ˆè¡¨æ ¼å½¢å¼ï¼‰
                    if projects:
                        st.markdown('<div class="section-title">ğŸ“¦ ç›¸å…³å¼€æºé¡¹ç›®</div>', unsafe_allow_html=True)
                        table_rows = []
                        for project in projects:
                            stats = get_github_stats(project)
                            table_rows.append(f'''
                            <tr>
                                <td><a href="{stats['url']}" target="_blank">{project}</a></td>
                                <td>
                                    <div class="project-stats">
                                        <span class="stat-item">â­ {stats['stars']}</span>
                                        <span class="stat-item">ğŸ”„ {stats['forks']}</span>
                                    </div>
                                </td>
                            </tr>
                            ''')
                        
                        st.markdown(f'''
                        <table class="project-table">
                            <thead>
                                <tr>
                                    <th>é¡¹ç›®åç§°</th>
                                    <th>ç»Ÿè®¡ä¿¡æ¯</th>
                                </tr>
                            </thead>
                            <tbody>
                                {"".join(table_rows)}
                            </tbody>
                        </table>
                        ''', unsafe_allow_html=True)
                    
                    # æ˜¾ç¤ºå…¬å¸æ ‡ç­¾
                    if companies:
                        st.markdown('<div class="section-title">ğŸ¢ ç›¸å…³å…¬å¸</div>', unsafe_allow_html=True)
                        tags_html = ''.join([f'<span class="company-tag">ğŸ¢ {company}</span>' for company in companies])
                        st.markdown(f'<div class="company-tags">{tags_html}</div>', unsafe_allow_html=True)
                    
                    # ä¿å­˜æ–‡ç« 
                    filename = save_article(news_title, news_content, projects, companies)
                    st.success(f"âœ… æ–‡ç« å·²ä¿å­˜: {filename}")
    
    else:  # st.session_state.nav == "list"
        st.markdown('<h1 class="main-title">ğŸ“š å…¨éƒ¨æ–‡ç« </h1>', unsafe_allow_html=True)
        articles = list_articles()
        
        if articles:
            # æ–‡ç« è¿‡æ»¤å’Œæœç´¢ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
            search = st.text_input("ğŸ” æœç´¢æ–‡ç« ", "")
            
            filtered_articles = articles
            if search:
                filtered_articles = [
                    article for article in articles 
                    if search.lower() in article['title'].lower()
                ]
            
            # æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨
            for article in filtered_articles:
                # è¯»å–æ–‡ç« å†…å®¹
                with open(article['path'], 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ˜¾ç¤ºæ–‡ç« å¡ç‰‡
                st.markdown(f'''
                <div class="article-card">
                    <h2 class="article-title">{article['title']}</h2>
                    <div class="article-meta">
                        <span class="meta-item">ğŸ“… å‘å¸ƒäº: {article["created_time"].strftime("%Y-%m-%d %H:%M")}</span>
                    </div>
                    <div class="article-content">{content}</div>
                </div>
                ''', unsafe_allow_html=True)
        else:
            st.info('ğŸ“ è¿˜æ²¡æœ‰ä¿å­˜çš„æ–‡ç« ï¼Œç‚¹å‡»"åˆ›å»ºæ–°æ–‡ç« "å¼€å§‹å†™ä½œå§ï¼')

if __name__ == "__main__":
    main()
